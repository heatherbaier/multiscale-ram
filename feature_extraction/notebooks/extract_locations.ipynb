{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207ccd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from tensorboard_logger import configure, log_value\n",
    "\n",
    "from model import RecurrentAttention\n",
    "from utils import AverageMeter\n",
    "\n",
    "from torchvision import transforms, utils, models\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "import utils\n",
    "\n",
    "from trainer import Trainer\n",
    "from config import get_config\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef1fc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inputs(impath):\n",
    "    to_tens = transforms.ToTensor()\n",
    "    return to_tens(Image.open(impath).convert('RGB')).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82677c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>muni_id</th>\n",
       "      <th>num_migrants</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>484001001</td>\n",
       "      <td>42055.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>484001002</td>\n",
       "      <td>4017.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>484001003</td>\n",
       "      <td>11992.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>484001004</td>\n",
       "      <td>762.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>484001005</td>\n",
       "      <td>7551.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>484032049</td>\n",
       "      <td>2487.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>484032050</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>484032051</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>484032052</td>\n",
       "      <td>2919.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>484032053</td>\n",
       "      <td>2119.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2331 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        muni_id  num_migrants class\n",
       "0     484001001       42055.0     1\n",
       "1     484001002        4017.0     1\n",
       "2     484001003       11992.0     1\n",
       "3     484001004         762.0     1\n",
       "4     484001005        7551.0     1\n",
       "...         ...           ...   ...\n",
       "2326  484032049        2487.0     1\n",
       "2327  484032050        2024.0     1\n",
       "2328  484032051        3084.0     1\n",
       "2329  484032052        2919.0     1\n",
       "2330  484032053        2119.0     1\n",
       "\n",
       "[2331 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = open(\"../../pooling/data/migration_data.json\",)\n",
    "mig_data = json.load(m)\n",
    "m.close()\n",
    "mig_data = pd.DataFrame.from_dict(mig_data, orient = 'index').reset_index()\n",
    "mig_data.columns = ['muni_id', 'num_migrants']\n",
    "q = 2\n",
    "mig_data['class'] = pd.qcut(mig_data['num_migrants'], q = q, labels = [i for i in range(q)])\n",
    "mig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0f0401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1166\n",
       "1    1165\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mig_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cdcd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_png_names(directory):\n",
    "    images = []\n",
    "    for i in os.listdir(directory):\n",
    "        try:\n",
    "            if os.path.isdir(os.path.join(directory, i)):\n",
    "                new_path = os.path.join(directory, i, \"pngs\")\n",
    "                image = os.listdir(new_path)[0]\n",
    "                images.append(os.path.join(directory, i, \"pngs\", image))\n",
    "        except:\n",
    "            pass\n",
    "    return images\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "image_names = get_png_names(\"../../attn/data/MEX/\")\n",
    "\n",
    "y_class, y_mig = [], []\n",
    "\n",
    "for i in image_names:\n",
    "        dta = mig_data[mig_data[\"muni_id\"] == i.split(\"/\")[5]]\n",
    "        if len(dta) != 0:\n",
    "            y_class.append(dta['class'].values[0])\n",
    "            y_mig.append(dta['num_migrants'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99202197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a65d0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_num = int(25 * .70)\n",
    "\n",
    "train_indices = random.sample(range(0, 25), train_num)\n",
    "val_indices = [i for i in range(0, 25) if i not in train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5f97d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# train = [(load_inputs(image_paths[i]).squeeze()[:, 0:28, 0:28], ys[i]) for i in range(0, 93)]\n",
    "# val = [(load_inputs(image_paths[i]).squeeze()[:, 0:28, 0:28], ys[i]) for i in range(93, 133)]\n",
    "\n",
    "# brighten = torchvision.transforms.functional.adjust_brightness(brightness_factor = 2)\n",
    "\n",
    "\n",
    "train = [(torchvision.transforms.functional.adjust_brightness(load_inputs(image_names[i]), brightness_factor = 2).squeeze(), y_class[i], y_mig[i]) for i in train_indices]\n",
    "val = [(torchvision.transforms.functional.adjust_brightness(load_inputs(image_names[i]), brightness_factor = 2).squeeze(), y_class[i], y_mig[i]) for i in val_indices]\n",
    "\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "val_dl = torch.utils.data.DataLoader(val, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1632edc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training:  17\n",
      "Num validation:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Num training: \", len(train_dl))\n",
    "print(\"Num validation: \", len(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d84fa9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import plot_images\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61f3caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, unparsed = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d1553a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config, (train_dl, val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2aa0d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./ckpt/ram_4_50x50_0.75_model_best.pth.tar\")\n",
    "checkpoint = checkpoint[\"model_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfc1dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from utils import denormalize, bounding_box\n",
    "\n",
    "def denormalize(T, coords):\n",
    "    return 0.5 * ((coords + 1.0) * T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64360fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exceeds(from_x, to_x, from_y, to_y, H, W):\n",
    "    \"\"\"Check whether the extracted patch will exceed\n",
    "    the boundaries of the image of size `T`.\n",
    "    \"\"\"\n",
    "    if (from_x < 0) or (from_y < 0) or (to_x > H) or (to_y > W):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def fix(from_x, to_x, from_y, to_y, H, W, size):\n",
    "\n",
    "    \"\"\"\n",
    "    Check whether the extracted patch will exceed\n",
    "    the boundaries of the image of size `T`.\n",
    "    If it will exceed, make a list of the offending reasons and fix them\n",
    "    \"\"\"\n",
    "\n",
    "    offenders = []\n",
    "\n",
    "    if (from_x < 0):\n",
    "        offenders.append(\"negative x\")\n",
    "    if from_y < 0:\n",
    "        offenders.append(\"negative y\")\n",
    "    if from_x > H:\n",
    "        offenders.append(\"from_x exceeds h\")            \n",
    "    if to_x > H:\n",
    "        offenders.append(\"to_x exceeds h\")\n",
    "    if from_y > W:\n",
    "        offenders.append(\"from_y exceeds w\")\n",
    "    if to_y > W:\n",
    "        offenders.append(\"to_y exceeds w\")            \n",
    "\n",
    "\n",
    "    if (\"from_y exceeds w\" in offenders) or (\"to_y exceeds w\" in offenders):\n",
    "        from_y, to_y = W - size, W\n",
    "\n",
    "    if (\"from_x exceeds h\" in offenders) or (\"to_x exceeds h\" in offenders):\n",
    "        from_x, to_x = H - size, H     \n",
    "\n",
    "    elif (\"negative x\" in offenders):\n",
    "        from_x, to_x = 0, 0 + size\n",
    "\n",
    "    elif (\"negative y\" in offenders):\n",
    "        from_y, to_y = 0, 0 + size            \n",
    "\n",
    "    return from_x, to_x, from_y, to_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b62ecb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../attn/data/MEX/484001008/pngs/484001008_2010_all_box484001008_MAY.png\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "../../attn/data/MEX/484004004/pngs/484004004_2010_all_box484004004_MAY.png\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "../../attn/data/MEX/484004001/pngs/484004001_2010_all_box484004001_MAY.png\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "../../attn/data/MEX/484005010/pngs/484005010_2010_all_box484005010_MAY.png\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "../../attn/data/MEX/484001005/pngs/484001005_2010_all_box484001005_MAY.png\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "locations_dict = {}\n",
    "\n",
    "for i in image_names[0:5]:\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    muni_id = i.split(\"/\")[5]\n",
    "    image = load_inputs(i)\n",
    "    locations = trainer.extract_locations(image, checkpoint)\n",
    "    \n",
    "    start = [denormalize(image.shape[2], l) for l in locations]\n",
    "    start = torch.cat([start[l].unsqueeze(0) for l in range(4)])\n",
    "        \n",
    "    B, C, H, W = image.shape\n",
    "    \n",
    "    size = int(min(H, W) / 5)\n",
    "    \n",
    "    end = start + size\n",
    "    \n",
    "#     print(start, end)\n",
    "    \n",
    "    coords_dict = {}\n",
    "    \n",
    "    for c in range(0, len(start)):\n",
    "        \n",
    "        from_coords = start[c]\n",
    "        to_coords = end[c]\n",
    "        \n",
    "        from_x = from_coords[0][0].item()\n",
    "        from_y = from_coords[0][1].item()\n",
    "        \n",
    "        to_x = to_coords[0][0].item()\n",
    "        to_y = to_coords[0][1] .item()   \n",
    "        \n",
    "        if exceeds(from_x = from_x, to_x = to_x, from_y = from_y, to_y = to_y, H = H, W = W):\n",
    "        \n",
    "            from_x, to_x, from_y, to_y = fix(from_x = from_x, to_x = to_x, from_y = from_y, to_y = to_y, H = H, W = W, size = size)\n",
    "        \n",
    "            print(\"yes\")\n",
    "        \n",
    "        coords_dict['glimpse' + str(c)] = {'from_x': from_x, 'to_x': to_x, 'from_y': from_y, 'to_y': to_y}\n",
    "    \n",
    "    locations_dict[i] = coords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c26b92e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'../../attn/data/MEX/484001008/pngs/484001008_2010_all_box484001008_MAY.png': {'glimpse0': {'from_x': 767,\n",
       "   'to_x': 881,\n",
       "   'from_y': 0.0,\n",
       "   'to_y': 114.0},\n",
       "  'glimpse1': {'from_x': 678.873291015625,\n",
       "   'to_x': 792.873291015625,\n",
       "   'from_y': 81.3102035522461,\n",
       "   'to_y': 195.31021118164062},\n",
       "  'glimpse2': {'from_x': 767,\n",
       "   'to_x': 881,\n",
       "   'from_y': 109.43223571777344,\n",
       "   'to_y': 223.43223571777344},\n",
       "  'glimpse3': {'from_x': 767, 'to_x': 881, 'from_y': 0.0, 'to_y': 114.0}},\n",
       " '../../attn/data/MEX/484004004/pngs/484004004_2010_all_box484004004_MAY.png': {'glimpse0': {'from_x': 1056,\n",
       "   'to_x': 1320,\n",
       "   'from_y': 0.0,\n",
       "   'to_y': 264.0},\n",
       "  'glimpse1': {'from_x': 1056,\n",
       "   'to_x': 1320,\n",
       "   'from_y': 346.80194091796875,\n",
       "   'to_y': 610.8019409179688},\n",
       "  'glimpse2': {'from_x': 1056, 'to_x': 1320, 'from_y': 0.0, 'to_y': 264.0},\n",
       "  'glimpse3': {'from_x': 859.859619140625,\n",
       "   'to_x': 1123.859619140625,\n",
       "   'from_y': 129.07077026367188,\n",
       "   'to_y': 393.0707702636719}},\n",
       " '../../attn/data/MEX/484004001/pngs/484004001_2010_all_box484004001_MAY.png': {'glimpse0': {'from_x': 2243,\n",
       "   'to_x': 2732,\n",
       "   'from_y': 888.11181640625,\n",
       "   'to_y': 1377.11181640625},\n",
       "  'glimpse1': {'from_x': 2031.197998046875,\n",
       "   'to_x': 2520.197998046875,\n",
       "   'from_y': 493.0560607910156,\n",
       "   'to_y': 982.0560302734375},\n",
       "  'glimpse2': {'from_x': 2243,\n",
       "   'to_x': 2732,\n",
       "   'from_y': 925.90673828125,\n",
       "   'to_y': 1414.90673828125},\n",
       "  'glimpse3': {'from_x': 2243,\n",
       "   'to_x': 2732,\n",
       "   'from_y': 450.1153564453125,\n",
       "   'to_y': 939.1153564453125}},\n",
       " '../../attn/data/MEX/484005010/pngs/484005010_2010_all_box484005010_MAY.png': {'glimpse0': {'from_x': 874,\n",
       "   'to_x': 1092,\n",
       "   'from_y': 77.40902709960938,\n",
       "   'to_y': 295.4090270996094},\n",
       "  'glimpse1': {'from_x': 874,\n",
       "   'to_x': 1092,\n",
       "   'from_y': 277.34222412109375,\n",
       "   'to_y': 495.34222412109375},\n",
       "  'glimpse2': {'from_x': 874,\n",
       "   'to_x': 1092,\n",
       "   'from_y': 11.94796085357666,\n",
       "   'to_y': 229.94796752929688},\n",
       "  'glimpse3': {'from_x': 800.1065673828125,\n",
       "   'to_x': 1018.1065673828125,\n",
       "   'from_y': 0.0,\n",
       "   'to_y': 218.0}},\n",
       " '../../attn/data/MEX/484001005/pngs/484001005_2010_all_box484001005_MAY.png': {'glimpse0': {'from_x': 763,\n",
       "   'to_x': 953,\n",
       "   'from_y': 211.4055938720703,\n",
       "   'to_y': 401.40557861328125},\n",
       "  'glimpse1': {'from_x': 763, 'to_x': 953, 'from_y': 0.0, 'to_y': 190.0},\n",
       "  'glimpse2': {'from_x': 763, 'to_x': 953, 'from_y': 0.0, 'to_y': 190.0},\n",
       "  'glimpse3': {'from_x': 763, 'to_x': 953, 'from_y': 0.0, 'to_y': 190.0}}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55e5e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand(2, 256).flatten(start_dim = 0).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7874f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample4.json\", \"w\") as outfile: \n",
    "    json.dump(locations_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5040c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = torch.rand(2, 256)\n",
    "# patch_dict = {}\n",
    "\n",
    "# for patch in range(0, test.shape[0]):\n",
    "#     cur_vals = list(test[patch].numpy())\n",
    "#     patch_dict[patch] = [str(i) for i in cur_vals]\n",
    "    \n",
    "# with open(\"sample.json\", \"w\") as outfile: \n",
    "#     json.dump(patch_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a73b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
